'''
from Phil Tabor's actor critic tutorial
'''
from keras import backend as K
from keras.layers import Dense, Input
from keras.models import Model, load_model
from keras.optimizers import Adam

import numpy as np
from collections import deque
import random
from operator import mul

class DiscreteAgent(object):
    def __init__(self, alpha, beta, gamma=0.99, n_actions=4,
                layer1_size=1024, layer2_size=512, input_dims=8):
        '''
        AC agent that contains all networks
        '''
        #agent parameters
        #for bellman
        self.gamma = gamma
        #actor learning rate
        self.alpha = alpha
        #critic learning rate
        self.beta = beta
        self.input_dims = input_dims
        self.fc1_dims = layer1_size
        self.fc2_dims = layer2_size
        self.n_actions = n_actions

        #build network and define vector for action space
        self.actor, self.critic, self.policy = self._build_actor_critic_network()
        self.action_space = [i for i in range(self.n_actions)]

    def _build_actor_critic_network(self):
        '''
        builds agent networks
        actor net approximates policy
        critic net calcs value of action 'V'

        actor and critic nets share middle layers, actor outputs probability,
        while critic outputs value. for simple envs, you can actually define
        2 totally separate networks (phil's words)
        '''
        input = Input(shape=(self.input_dims,))

        #used in the loss function, aka advantage
        delta = Input(shape=[1])

        #shared layers
        dense1 = Dense(self.fc1_dims, activation='relu')(input)
        dense2 = Dense(self.fc2_dims, activation='relu')(dense1)
        dense3 = Dense(256, activation='relu')(dense2)
        dense4 = Dense(128, activation='relu')(dense3)

        #prob output uses softmax activation
        probs = Dense(self.n_actions, activation='softmax')(dense4)

        #critic net output
        values = Dense(1, activation='linear')(dense4)


        def custom_loss(y_true, y_pred):
            '''
            custom loss, outputs a function
            '''
            #clip so we dont log(0)
            out = K.clip(y_pred*y_true, 1e-10, 1-1e-10)

            log_lik = K.log(out)

            return K.sum(-log_lik*delta)

        #actor, used for training
        actor = Model(input=[input, delta], output=[probs])
        actor.compile(optimizer=Adam(lr=self.alpha), loss=custom_loss)

        #critic network
        critic = Model(input=[input], output=[values])
        critic.compile(optimizer=Adam(lr=self.beta), loss='mean_squared_error')

        #prediction only actor network, no compilation needed (no training)
        policy = Model(input=[input], output=[probs])

        return actor, critic, policy

    def choose_action(self, observation):
        #add new axis to obs vector
        state = observation[np.newaxis, :]

        #get probs
        probabilities = self.policy.predict(state)[0]
        #print(probabilities)
        #sample probability generated by actor
        action = np.random.choice(self.action_space, p=probabilities)

        return action

    def learn(self, state, action, reward, state_, done):
        state = state[np.newaxis, :]
        state_ = state_[np.newaxis, :]

        critic_value_ = self.critic.predict(state_)
        critic_value = self.critic.predict(state)

        #compute target and advantage
        target = reward + self.gamma*critic_value_*(1-int(done))
        delta = target - critic_value

        #convert action to one hot encoding
        actions = np.zeros([1, self.n_actions])
        actions[0, action] = 1.0
        #print(actions)

        self.actor.fit([state, delta], actions, verbose=0)
        self.critic.fit(state, target, verbose=0)

    def load_policy(self, model_name):
        '''load a saved policy'''
        self.policy = load_model(model_name)
        print('policy {} loaded'.format(model_name))

class DiscreteReplayAgent(object):
    def __init__(self, alpha, beta, gamma=0.99, n_actions=4,
                layer1_size=1024, layer2_size=512, input_dims=8,
                memory_size=10000, minibatch_size=32):
        '''
        AC agent with replay memory
        '''
        #agent parameters
        #for bellman
        self.gamma = gamma
        #actor learning rate
        self.alpha = alpha
        #critic learning rate
        self.beta = beta
        self.input_dims = input_dims
        self.fc1_dims = layer1_size
        self.fc2_dims = layer2_size
        self.n_actions = n_actions

        #replay memory
        self.minibatch_size = minibatch_size
        self.memory_size = memory_size
        self.memory = deque(maxlen=memory_size)

        #build network and define vector for action space
        self.actor, self.critic, self.policy = self._build_actor_critic_network()
        self.action_space = [i for i in range(self.n_actions)]

    def _build_actor_critic_network(self):
        '''
        builds agent networks
        actor net approximates policy
        critic net calcs value of action 'V'

        actor and critic nets share middle layers, actor outputs probability,
        while critic outputs value. for simple envs, you can actually define
        2 totally separate networks (phil's words)
        '''
        input = Input(shape=(self.input_dims,))

        #used in the loss function, aka advantage
        delta = Input(shape=[1])

        #shared layers
        dense1 = Dense(self.fc1_dims, activation='relu')(input)
        dense2 = Dense(self.fc2_dims, activation='relu')(dense1)

        #prob output uses softmax activation
        probs = Dense(self.n_actions, activation='softmax')(dense2)

        #critic net output
        values = Dense(1, activation='linear')(dense2)


        def custom_loss(y_true, y_pred):
            '''
            custom loss, outputs a function
            '''
            #clip so we dont log(0)
            out = K.clip(y_pred*y_true, 1e-10, 1-1e-10)

            log_lik = K.log(out)

            return K.sum(-log_lik*delta)

        #actor, used for training
        actor = Model(input=[input, delta], output=[probs])
        actor.compile(optimizer=Adam(lr=self.alpha), loss=custom_loss)

        #critic network
        critic = Model(input=[input], output=[values])
        critic.compile(optimizer=Adam(lr=self.beta), loss='mean_squared_error')

        #prediction only actor network, no compilation needed (no training)
        policy = Model(input=[input], output=[probs])

        return actor, critic, policy

    def choose_action(self, observation):
        #add new axis to obs vector
        state = observation[np.newaxis, :]

        #get probs
        probabilities = self.policy.predict(state)[0]
        #print(probabilities)
        #sample probability generated by actor
        action = np.random.choice(self.action_space, p=probabilities)

        return action

    def remember(self, state, action, reward, state_, done):
        '''add to replay memory'''
        memory_unit = [state, action, reward, state_, done]
        self.memory.append(memory_unit)

    def sample_from_memory(self):
        '''randomly pick a minibatch from replay memory'''
        minibatch = random.sample(self.memory, self.minibatch_size)
        return minibatch

    def learn(self):
        '''update networks using memory'''
        #exit function if theres not enough replay memory
        if len(self.memory) < self.minibatch_size:
            print('not training, still accumulating experience')
            return

        #sample minibatch and unpack
        minibatch = self.sample_from_memory()
        state = []
        state_ = []
        done = []
        reward = []
        action = []
        for i in range(self.minibatch_size):
            state.append(minibatch[i][0])
            action.append(minibatch[i][1])
            reward.append(minibatch[i][2])
            state_.append(minibatch[i][3])
            done.append(minibatch[i][4])

        state = np.asarray(state)
        action = np.asarray(action)
        reward = np.asarray(reward)
        state_ = np.asarray(state_)

        #get value for the batch
        critic_value_ = np.squeeze(self.critic.predict(state_))
        critic_value = np.squeeze(self.critic.predict(state))

        #compute target and advantage
        not_done = list(map(int.__sub__, [1]*self.minibatch_size, list(map(int, done))))
        target = reward + self.gamma*np.multiply(critic_value_, not_done)
        delta = target - critic_value

        #convert action to one hot encoding
        actions = np.zeros([self.minibatch_size, self.n_actions])
        for a in range(self.minibatch_size):
            actions[a, action[a]] = 1.0

        print(actions)

        self.actor.fit([state, delta], actions, verbose=0)
        self.critic.fit(state, target, verbose=0)

    def load_policy(self, model_name):
        '''load a saved policy'''
        self.policy = load_model(model_name)
        print('policy {} loaded'.format(model_name))
